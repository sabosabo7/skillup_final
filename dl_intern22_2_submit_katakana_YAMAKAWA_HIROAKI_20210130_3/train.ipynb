{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from common.layers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from common.optimizer import *\n",
    "import pickle\n",
    "import sklearn.preprocessing as sp\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デフォルトデータセットを読む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "raw_data.shape= (3000, 1, 28, 28)\nraw_label.shape= (3000, 15)\n"
     ]
    }
   ],
   "source": [
    "raw_data = np.load(\"./data/train_data.npy\")\n",
    "raw_label = np.load(\"./data/train_label.npy\")\n",
    "raw_data = raw_data[:raw_data.shape[0]//1000*1000]\n",
    "raw_label = raw_label[:raw_label.shape[0]//1000*1000]\n",
    "print(\"raw_data.shape=\", raw_data.shape)\n",
    "print(\"raw_label.shape=\", raw_label.shape)\n",
    "# 正規化\n",
    "raw_data = (raw_data - raw_data.min()) / raw_data.max()\n",
    "raw_data = raw_data.astype('float32')"
   ]
  },
  {
   "source": [
    "## データ拡張済みデータセットを読む"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_data.shape= (30000, 1, 28, 28)\ntrain_label.shape= (30000, 15)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load(\"./data/integ_data7.npy\")\n",
    "train_label = np.load(\"./data/integ_label7.npy\")\n",
    "train_data = train_data[:train_data.shape[0]//1000*1000]\n",
    "train_label = train_label[:train_label.shape[0]//1000*1000]\n",
    "print(\"train_data.shape=\", train_data.shape)\n",
    "print(\"train_label.shape=\", train_label.shape)\n",
    "# 正規化\n",
    "train_data = (train_data - train_data.min()) / train_data.max()\n",
    "train_data = train_data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_data.shape= (30000, 784)\n"
     ]
    }
   ],
   "source": [
    "# 配列形式変更\n",
    "train_data = train_data.reshape(-1, 28*28)\n",
    "print(\"train_data.shape=\", train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainとtestに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21000, 784) (9000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_label,test_size=0.3, random_state=1234, shuffle=True)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,1,28,28)\n",
    "X_test = X_test.reshape(-1,1,28,28)\n",
    "train_data = train_data.reshape(-1,1, 28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21000, 1, 28, 28) (9000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各種関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(i):\n",
    "    with open(\"katakana_deep_d70_ep\"+str(i)+\"_w.pickle\", \"wb\") as f:\n",
    "        pickle.dump(tnet.params, f)\n",
    "\n",
    "    c={\n",
    "        \"Batch_c1\":[tnet.layers[\"Batch_c1\"].moving_mean,tnet.layers[\"Batch_c1\"].moving_var],\n",
    "        # \"Batch_c2\":[tnet.layers[\"Batch_c2\"].moving_mean,tnet.layers[\"Batch_c2\"].moving_var],\n",
    "        \"Batch_c3\":[tnet.layers[\"Batch_c3\"].moving_mean,tnet.layers[\"Batch_c3\"].moving_var],\n",
    "        # \"Batch_c4\":[tnet.layers[\"Batch_c4\"].moving_mean,tnet.layers[\"Batch_c4\"].moving_var],\n",
    "        \"Batch_c5\":[tnet.layers[\"Batch_c5\"].moving_mean,tnet.layers[\"Batch_c5\"].moving_var],\n",
    "        \"Batch_a1\":[tnet.layers[\"Batch_a1\"].moving_mean,tnet.layers[\"Batch_a1\"].moving_var]\n",
    "    }\n",
    "    with open(\"katakana_deep_d70_ep\"+str(i)+\"_c.pickle\", \"wb\") as f:\n",
    "        pickle.dump(c, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_acc(d,l,model): \n",
    "    data_size = len(d)\n",
    "    batch_size = 1000\n",
    "    minibatch_num = np.ceil( data_size / batch_size).astype(int) \n",
    "    li_loss = []\n",
    "    li_accuracy = []\n",
    "    li_num = []\n",
    "    index = np.arange(data_size)\n",
    "    \n",
    "    for mn in range(minibatch_num):\n",
    "        # print(mn)\n",
    "        mask = index[batch_size*mn:batch_size*(mn+1)]        \n",
    "        data = d[mask]\n",
    "        label = l[mask]\n",
    "        y_pre=model.predict(data,train_flg=False)\n",
    "        loss = model.last_layer.forward(y_pre,label)\n",
    "        y_pre= np.argmax(y_pre,axis=1)\n",
    "        label= np.argmax(label,axis=1)\n",
    "        accuracy = np.sum(y_pre==label)/data.shape[0]\n",
    "        # print(loss, accuracy)\n",
    "        \n",
    "        li_loss.append(loss)\n",
    "        li_accuracy.append(accuracy)\n",
    "        li_num.append(len(data))\n",
    "\n",
    "    ave_loss = np.dot(li_loss, li_num) / np.sum(li_num)\n",
    "    print('loss:', ave_loss)\n",
    "    ave_accuracy = np.dot(li_accuracy, li_num) / np.sum(li_num)\n",
    "    print('accuracy:', ave_accuracy)\n",
    "    return ave_loss,ave_accuracy"
   ]
  },
  {
   "source": [
    "## 学習"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnet = DeepConvNet(input_dim=(1, 28, 28), hidden_size=100, output_size=15, weight_init_std=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Conv1\nBatch_c1\nReLU1\nPool1\nConv3\nBatch_c3\nReLU3\nPool2\nConv5\nBatch_c5\nReLU5\nAffine1\nBatch_a1\nAffine2\n\nW_c1    \t(32, 1, 3, 3)\nb_c1    \t(32,)\ngamma_c1    \t(32,)\nbeta_c1    \t(32,)\nW_c3    \t(64, 32, 3, 3)\nb_c3    \t(64,)\ngamma_c3    \t(64,)\nbeta_c3    \t(64,)\nW_c5    \t(64, 64, 3, 3)\nb_c5    \t(64,)\ngamma_c5    \t(64,)\nbeta_c5    \t(64,)\nW_a1    \t(1600, 100)\nb_a1    \t(100,)\ngamma_a1    \t(100,)\nbeta_a1    \t(100,)\nW_a2    \t(100, 15)\nb_a2    \t(15,)\n"
     ]
    }
   ],
   "source": [
    "for l in tnet.layers.keys():\n",
    "    print(l)\n",
    "for p in tnet.params.keys():\n",
    "    print(p+\"    \\t\"+str(tnet.params[p].shape))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "optimizer = Adam()\n",
    "epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch=5\n",
      "it= 0\n",
      "it= 1\n",
      "it= 2\n",
      "it= 3\n",
      "it= 4\n",
      "it= 5\n",
      "it= 6\n",
      "it= 7\n",
      "it= 8\n",
      "it= 9\n",
      "it= 10\n",
      "it= 11\n",
      "it= 12\n",
      "it= 13\n",
      "it= 14\n",
      "it= 15\n",
      "it= 16\n",
      "it= 17\n",
      "it= 18\n",
      "it= 19\n",
      "it= 20\n",
      "###  train  ###\n",
      "loss: 0.03706128651223221\n",
      "accuracy: 0.9999047619047619\n",
      "\n",
      "###  test  ###\n",
      "loss: 0.04894745361242961\n",
      "accuracy: 0.9973333333333333\n",
      "epoch=6\n",
      "it= 0\n",
      "it= 1\n",
      "it= 2\n",
      "it= 3\n",
      "it= 4\n",
      "it= 5\n",
      "it= 6\n",
      "it= 7\n",
      "it= 8\n",
      "it= 9\n",
      "it= 10\n",
      "it= 11\n",
      "it= 12\n",
      "it= 13\n",
      "it= 14\n",
      "it= 15\n",
      "it= 16\n",
      "it= 17\n",
      "it= 18\n",
      "it= 19\n",
      "it= 20\n",
      "###  train  ###\n",
      "loss: 0.03360810556306863\n",
      "accuracy: 1.0\n",
      "\n",
      "###  test  ###\n",
      "loss: 0.046386756864268776\n",
      "accuracy: 0.998\n",
      "epoch=7\n",
      "it= 0\n",
      "it= 1\n",
      "it= 2\n",
      "it= 3\n",
      "it= 4\n",
      "it= 5\n",
      "it= 6\n",
      "it= 7\n",
      "it= 8\n",
      "it= 9\n",
      "it= 10\n",
      "it= 11\n",
      "it= 12\n",
      "it= 13\n",
      "it= 14\n",
      "it= 15\n",
      "it= 16\n",
      "it= 17\n",
      "it= 18\n",
      "it= 19\n",
      "it= 20\n",
      "###  train  ###\n",
      "loss: 0.018265662115968737\n",
      "accuracy: 1.0\n",
      "\n",
      "###  test  ###\n",
      "loss: 0.0291746621704011\n",
      "accuracy: 0.9981111111111111\n",
      "epoch=8\n",
      "it= 0\n",
      "it= 1\n",
      "it= 2\n",
      "it= 3\n",
      "it= 4\n",
      "it= 5\n",
      "it= 6\n",
      "it= 7\n",
      "it= 8\n",
      "it= 9\n",
      "it= 10\n",
      "it= 11\n",
      "it= 12\n",
      "it= 13\n",
      "it= 14\n",
      "it= 15\n",
      "it= 16\n",
      "it= 17\n",
      "it= 18\n",
      "it= 19\n",
      "it= 20\n",
      "###  train  ###\n",
      "loss: 0.014749201000542857\n",
      "accuracy: 1.0\n",
      "\n",
      "###  test  ###\n",
      "loss: 0.024573820977009796\n",
      "accuracy: 0.9987777777777778\n",
      "epoch=9\n",
      "it= 0\n",
      "it= 1\n",
      "it= 2\n",
      "it= 3\n",
      "it= 4\n",
      "it= 5\n",
      "it= 6\n",
      "it= 7\n",
      "it= 8\n",
      "it= 9\n",
      "it= 10\n",
      "it= 11\n",
      "it= 12\n",
      "it= 13\n",
      "it= 14\n",
      "it= 15\n",
      "it= 16\n",
      "it= 17\n",
      "it= 18\n",
      "it= 19\n",
      "it= 20\n",
      "###  train  ###\n",
      "loss: 0.01537340000247865\n",
      "accuracy: 1.0\n",
      "\n",
      "###  test  ###\n",
      "loss: 0.023655391028879518\n",
      "accuracy: 0.9992222222222222\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "# 繰り返し回数\n",
    "xsize = X_train.shape[0]\n",
    "iter_num = np.ceil(xsize / batch_size).astype(np.int)\n",
    "\n",
    "\n",
    "epochs=10\n",
    "while epoch<epochs:\n",
    "    \n",
    "    # シャッフル\n",
    "    idx = np.arange(xsize)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    for it in range(iter_num):\n",
    "        \"\"\"\n",
    "        ランダムなミニバッチを順番に取り出す\n",
    "        \"\"\"\n",
    "        print(\"it=\", it)\n",
    "        mask = idx[batch_size*it : batch_size*(it+1)]\n",
    "    \n",
    "        # ミニバッチの生成\n",
    "        x_ = X_train[mask]\n",
    "        y_ = y_train[mask]\n",
    "        \n",
    "        # 勾配の計算\n",
    "        grads = tnet.gradient(x_, y_)\n",
    "\n",
    "        # パラメータの更新\n",
    "        optimizer.update(tnet.params, grads)\n",
    "\n",
    "    ## 学習経過の記録\n",
    "    \n",
    "  \n",
    "    # 訓練データにおけるloss,acc\n",
    "    print(\"###  train  ###\")\n",
    "    loss,acc=calc_loss_acc(X_train,y_train,tnet)\n",
    "    train_loss.append(loss)\n",
    "    train_accuracy.append(acc)\n",
    "    print()\n",
    "    # テストデータにおけるloss,acc\n",
    "    print(\"###  test  ###\")\n",
    "    loss,acc=calc_loss_acc(X_test,y_test,tnet)\n",
    "    test_loss.append(loss)\n",
    "    test_accuracy.append(acc)\n",
    "\n",
    "    \n",
    "    # save_pickle(epoch)\n",
    "    \n",
    "    if test_accuracy[-1]==1:\n",
    "        break\n",
    "    epoch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(epoch-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_loss_acc(X_train,y_train,tnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_loss,label=\"test\")\n",
    "plt.plot(train_loss,label=\"train\")\n",
    "plt.legend()\n",
    "plt.ylim([0,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_accuracy,label=\"test\")\n",
    "plt.plot(train_accuracy,label=\"train\")\n",
    "plt.legend()\n",
    "plt.ylim([0.95,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルの復元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"katakana_deep_d70_ep9_w.pickle\", \"rb\") as f:\n",
    "    w=pickle.load(f)\n",
    "with open(\"katakana_deep_d70_ep9_c.pickle\", \"rb\") as f:\n",
    "    c=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnet2=DeepConvNet(input_dim=(1, 28, 28),param=w,const=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.08111119,  0.00194493,  0.01839371,  0.05980254,  0.02373021,\n",
       "        0.00427221,  0.03977364,  0.02689135,  0.00602421,  0.03666823,\n",
       "        0.01637456,  0.00475368,  0.02127838,  0.00391997,  0.00314332,\n",
       "       -0.01683814,  0.01010332, -0.03003121, -0.01890055,  0.00879357,\n",
       "       -0.01975108,  0.0219052 ,  0.06646518, -0.00017839,  0.02796989,\n",
       "        0.00692618,  0.01119468,  0.00215044,  0.00880133,  0.01295128,\n",
       "        0.07349581, -0.06492941])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "tnet2.layers[\"Batch_c1\"].moving_mean"
   ]
  },
  {
   "source": [
    "## 誤答表示"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_fail(x,y,t,cnt):\n",
    "    kana=[\"a\",\"i\",\"u\",\"e\",\"o\",\"ka\",\"ki\",\"ku\",\"ke\",\"ko\",\"sa\",\"si\",\"su\",\"se\",\"so\"]\n",
    "    for _x,_y,_t in zip(x,y,t):\n",
    "        __t,__y=np.argmax(_t),np.argmax(_y)\n",
    "        if __y!=__t:\n",
    "            cnt+=1\n",
    "            print(\"cnt:\",cnt,\"label:\",kana[__t],\"pre:\",kana[__y])\n",
    "            plt.imshow(_x[0,:,:],cmap=\"gray\")\n",
    "            plt.show()\n",
    "    return cnt\n",
    "\n",
    "def check_fail(x,t,m):\n",
    "    index = np.arange(x.shape[0])\n",
    "    cnt=0\n",
    "    for i in range(x.shape[0]//100):\n",
    "        print(\"No.\",i*1000,\"~\")\n",
    "        mask=index[i*1000:(i+1)*1000]\n",
    "        _x=x[mask]\n",
    "        _t=t[mask]\n",
    "        _pre_x=m.predict(_x,train_flg=False)\n",
    "        cnt=_check_fail(_x,_pre_x,_t,cnt)\n",
    "        if cnt>100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}